{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_api_key\n",
    "API_KEY = load_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool predefinida por LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"¿En qué fecha has sido entrenado?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"¿Quien es el presidente de Estados Unidos?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acceso a la [documentación de WikipediaApiWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.wikipedia.WikipediaAPIWrapper.html#langchain_community.utilities.wikipedia.WikipediaAPIWrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(\n",
    "    lang=\"es\",\n",
    "    top_k_results=3,\n",
    "    # doc_content_chars_max=1000\n",
    ")\n",
    "wikipedia_tool = WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a probar a hacer directamente una búsqueda manualmente en wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Name: {wikipedia_tool.name}\")\n",
    "print(f\"Description: {wikipedia_tool.description}\")\n",
    "print(f\"Args: {wikipedia_tool.args}\")\n",
    "print(\"----------------------------------------------\")\n",
    "\n",
    "print(wikipedia_tool.invoke({\"query\": \"python\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a unir `wikipedia_tool` con nuestro LLM y así poder buscar automáticamente esta información. \n",
    "\n",
    "El orden será:\n",
    "1. Usar nuestro LLM para escribir los parámetros de la tool\n",
    "2. Ejecutar la tool (y así añadir la información necesaria al contexto)\n",
    "3. Volver a llamar al LLM con todo el contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = {\"wikipedia\": wikipedia_tool}\n",
    "\n",
    "llm_with_tools = llm.bind_tools([t for t in tools.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **Nota**: No todos los modelos de LLM tienen la capacidad de ejecutar Tools!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "messages = []\n",
    "prompt = HumanMessage(\"Quién es el presidente de Estados Unidos?\")\n",
    "messages.append(prompt)\n",
    "\n",
    "# 1. Generar petición a Tool\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "print(f\"ContentString: {ai_msg.content}\")\n",
    "print(f\"ToolCalls: {ai_msg.tool_calls}\")\n",
    "messages.append(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Realizar petición a la Tool y guardar en ChatHistory\n",
    "if ai_msg.tool_calls:\n",
    "    for tool_call in ai_msg.tool_calls:\n",
    "        tool_msg = tools[tool_call[\"name\"]].invoke(tool_call)\n",
    "        # print(f\"ContentString: {tool_msg}\")\n",
    "        messages.append(tool_msg)\n",
    "display(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Repetir llamada, incluyendo salida de Tool\n",
    "response = llm_with_tools.invoke(messages)\n",
    "messages.append(response)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in messages:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool personal\n",
    "\n",
    "A continuación vamos a crear una custom Tool. Para ello, vamos a definir una función de Python y la decoraremos con el decorador `@tool`\n",
    "\n",
    "❗❗ OJO: Para poder crear una Tool a partir de una función de Python debemos proporcionar los typing annotations y un docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiplicar_numeros(num1: int, num2: int) -> int:\n",
    "    \"\"\"\n",
    "    Multiplies two integers and returns the result.\n",
    "\n",
    "    Args:\n",
    "        num1 (int): The first integer.\n",
    "        num2 (int): The second integer.\n",
    "\n",
    "    Returns:\n",
    "        int: The result of multiplying num1 and num2.\n",
    "    \"\"\"\n",
    "    return num1 * num2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Name: {multiplicar_numeros.name}\")\n",
    "print(f\"Description: {multiplicar_numeros.description}\")\n",
    "print(f\"Args: {multiplicar_numeros.args}\")\n",
    "print(\"----------------------------------------------\")\n",
    "\n",
    "print(multiplicar_numeros.invoke({\"num1\": 12, \"num2\": 2}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = {\"wikipedia\": wikipedia_tool, \"multiplicar_numeros\": multiplicar_numeros}\n",
    "\n",
    "llm_with_tools = llm.bind_tools([t for t in tools.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "messages = []\n",
    "prompt = HumanMessage(\"Dame el resultado de multiplicar 8712387821763 y 12389749833\")\n",
    "messages.append(prompt)\n",
    "\n",
    "# 1. Generar petición a Tool\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "print(f\"ContentString: {ai_msg.content}\")\n",
    "print(f\"ToolCalls: {ai_msg.tool_calls}\")\n",
    "messages.append(ai_msg)\n",
    "\n",
    "# 2. Realizar petición a la Tool y guardar en ChatHistory\n",
    "if ai_msg.tool_calls:\n",
    "    for tool_call in ai_msg.tool_calls:\n",
    "        tool_msg = tools[tool_call[\"name\"]].invoke(tool_call)\n",
    "        # print(f\"ContentString: {tool_msg}\")\n",
    "        messages.append(tool_msg)\n",
    "display(messages)\n",
    "\n",
    "# 3. Repetir llamada, incluyendo salida de Tool\n",
    "response = llm_with_tools.invoke(messages)\n",
    "messages.append(response)\n",
    "response\n",
    "\n",
    "for msg in messages:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{8712387821763 * 12389749833:_}\"  # Resultado correcto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como el LLM tiene varias Tools, elegirá la más adecuada en cada ocasión "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools.invoke(\"Dame el resultado de multiplicar 8712387821763 y 12389749833\").tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools.invoke(\"Quién es el presidente de Estados Unidos?\").tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools.invoke(\"Hola!\").tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools.invoke(\"Hola!\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.invoke(\"Crea un poema sobre LangChain\").pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in llm.stream(\"Crea un poema sobre LangChain\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
