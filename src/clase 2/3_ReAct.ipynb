{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper: [https://arxiv.org/pdf/2210.03629](https://arxiv.org/pdf/2210.03629)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_api_key\n",
    "API_KEY = load_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación \"from scratch\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "API_KEY = API_KEY if API_KEY else os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "OPENAI_CLIENT = OpenAI(\n",
    "    api_key=API_KEY,\n",
    ")\n",
    "\n",
    "OPENAI_MODEL = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_two_elements(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Computes the sum of two integers.\n",
    "\n",
    "    Args:\n",
    "        a (int): The first integer to be summed.\n",
    "        b (int): The second integer to be summed.\n",
    "\n",
    "    Returns:\n",
    "        int: The sum of `a` and `b`.\"\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "def multiply_two_elements(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Computes the product of two integers.\n",
    "\n",
    "    Args:\n",
    "        a (int): The first integer to be multiplied.\n",
    "        b (int): The second integer to be multiplied.\n",
    "\n",
    "    Returns:\n",
    "        int: The product of `a` and `b`.\"\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "def compute_log(x: int) -> float:\n",
    "    \"\"\"\n",
    "    Computes the natural logarithm of a number.\n",
    "\n",
    "    Args:\n",
    "        x (int): The number to compute the logarithm of.\n",
    "\n",
    "    Returns:\n",
    "        float: The natural logarithm of `x`.\"\n",
    "    \"\"\"\n",
    "    if x <= 0:\n",
    "        raise ValueError(\"The input must be a positive number.\")\n",
    "\n",
    "    return math.log(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora definiremos un System Prompt que vamos a emplear en todas las llamadas a la API de OpenAI. Fijaos que dentro de las etiquetas de `<tools></tools>` hemos introducido las definiciones de las funciones. Esto es exactamente igual a lo que hará LangChain si creamos nuestra propia Tool o si usamos algunas de las Tools de `langchain_community`.\n",
    "\n",
    "Además, en la sesión de ejemplo, hemos definido una iteración del bucle de ReAct:\n",
    "\n",
    "pensamiento -> acción (tool) -> observación -> pensamiento -> acción (tool) -> observación -> pensamiento ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"Eres un modelo de IA que llama a funciones. Operas ejecutando un ciclo con los siguientes pasos:\n",
    "\n",
    "1. Pensamiento\n",
    "2. Acción\n",
    "3. Observación\n",
    "\n",
    "Se te proporcionan definiciones de funciones dentro de etiquetas XML `<tools></tools>`.  \n",
    "Puedes llamar a una o más funciones para ayudar con la consulta del usuario. \n",
    "No hagas suposiciones sobre qué valores debes introducir en las funciones.\n",
    "\n",
    "Para cada llamada de función, devuelve un objeto JSON con el nombre de la función y los argumentos dentro de las etiquetas XML `<tool_call></tool_call>` como sigue:\n",
    "\n",
    "<tool_call> {\"name\": <function-name>, \"arguments\": <args-dict>, \"id\": <monotonically-increasing-id>} </tool_call>\n",
    "\n",
    "Aquí están las herramientas / acciones disponibles:\n",
    " \n",
    "<tools>\n",
    "{\"name\": \"sum_two_elements\", \"description\": \"\\n    Computes the sum of two integers.\\n\\n    Args:\\n    a (int): The first integer to be summed.\\n    b (int): The second integer to be summed.\\n\\n    Returns:\\n    int: The sum of `a` and `b`.\\n\"}\n",
    "{\"name\": \"multiply_two_elements\", \"description\": \"\\n    Multiplies two integers.\\n\\n    Args:\\n    a (int): The first integer.\\n    b (int): The second integer.\\n\\n    Returns:\\n    int: The product of `a` and `b`.\\n\"}\n",
    "{\"name\": \"compute_log\", \"description\": \"\\n    Computes the logarithm of an integer `x` with an optional base.\\n    Args:\\n    x (int): The number to compute the logarithm for.\\n    base (int, optional): The logarithm base, default is `e`.\\n    Returns:\\n    float: The logarithm of `x` in the given base.\\n\"}\n",
    "</tools>\n",
    "\n",
    "Ejemplo de sesión:\n",
    "\n",
    "<question>¿Cuál es la temperatura actual en Madrid?</question>\n",
    "<thought>Necesito obtener el clima actual en Madrid</thought>\n",
    "<tool_call>{\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"Madrid\", \"unit\": \"celsius\"}, \"id\": 0}</tool_call>\n",
    "\n",
    "Te llamarán nuevamente con esto:\n",
    "\n",
    "<observation>{0: {\"temperature\": 25, \"unit\": \"celsius\"}}</observation>\n",
    "\n",
    "Entonces debes producir la salida:\n",
    "\n",
    "<response>La temperatura actual en Madrid es de 25 grados Celsius</response>\n",
    "\n",
    "Restricciones adicionales:\n",
    "\n",
    "Si el usuario te pregunta algo que no esté relacionado con ninguna de las herramientas anteriores, \n",
    "responde libremente encerrando tu respuesta con etiquetas <response></response>.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las restricciones adicionales son importantes para que no intente ejecutar las tools en todas las iteraciones del bucle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_QUESTION = \"\"\"Quiero calcular la suma de 1234 y 5678 y multiplicar el resultado por 5.\n",
    "Luego quiero calcular el logaritmo natural de ese resultado.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": SYSTEM_PROMPT\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"<question>{USER_QUESTION}</question>\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = OPENAI_CLIENT.chat.completions.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    messages=chat_history\n",
    ").choices[0].message.content\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": output\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tag_content(text: str, tag: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the content of a tag from a text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to extract the tag content from.\n",
    "        tag (str): The tag to extract the content from.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the tag.\n",
    "    \"\"\"\n",
    "    tag_pattern = f\"<{tag}>(.*?)</{tag}>\"\n",
    "    matched_content = re.findall(tag_pattern, text, re.DOTALL)\n",
    "    return [json.loads(content) for content in matched_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_tag_content(output, tag=\"tool_call\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = extract_tag_content(output, tag=\"tool_call\")[0][\"arguments\"]\n",
    "tool_result = sum_two_elements(**arguments)\n",
    "tool_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"<observation>{tool_result}</observation>\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bucle 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = OPENAI_CLIENT.chat.completions.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    messages=chat_history\n",
    ").choices[0].message.content\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": output\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = extract_tag_content(output, tag=\"tool_call\")[0][\"arguments\"]\n",
    "tool_result = multiply_two_elements(**arguments)\n",
    "tool_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"<observation>{tool_result}</observation>\"\n",
    "})\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bucle 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = OPENAI_CLIENT.chat.completions.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    messages=chat_history\n",
    ").choices[0].message.content\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": output\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = extract_tag_content(output, tag=\"tool_call\")[0][\"arguments\"]\n",
    "tool_result = compute_log(**arguments)\n",
    "tool_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"<observation>{tool_result}</observation>\"\n",
    "})\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bucle 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = OPENAI_CLIENT.chat.completions.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    messages=chat_history\n",
    ").choices[0].message.content\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queréis ver como quedaría un codigo más profesional, podéis ver el siguiente ejemplo: [Otro ejemplo de implementación de ReAct](https://medium.com/the-ai-forum/create-a-react-agent-from-scratch-without-using-any-llm-frameworks-only-with-python-and-groq-c10510d32dbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReAct Agent con LangGraph\n",
    "\n",
    "## Ejemplo 1 con LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "import math\n",
    "\n",
    "@tool\n",
    "def sum_two_elements(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Computes the sum of two integers.\n",
    "\n",
    "    Args:\n",
    "        a (int): The first integer to be summed.\n",
    "        b (int): The second integer to be summed.\n",
    "\n",
    "    Returns:\n",
    "        int: The sum of `a` and `b`.\"\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply_two_elements(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Computes the product of two integers.\n",
    "\n",
    "    Args:\n",
    "        a (int): The first integer to be multiplied.\n",
    "        b (int): The second integer to be multiplied.\n",
    "\n",
    "    Returns:\n",
    "        int: The product of `a` and `b`.\"\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def compute_log(x: int) -> float:\n",
    "    \"\"\"\n",
    "    Computes the natural logarithm of a number.\n",
    "\n",
    "    Args:\n",
    "        x (int): The number to compute the logarithm of.\n",
    "\n",
    "    Returns:\n",
    "        float: The natural logarithm of `x`.\"\n",
    "    \"\"\"\n",
    "    if x <= 0:\n",
    "        raise ValueError(\"The input must be a positive number.\")\n",
    "\n",
    "    return math.log(x)\n",
    "\n",
    "tools = [sum_two_elements, multiply_two_elements, compute_log]\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "agent_executor = create_react_agent(llm, tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_executor.invoke(\n",
    "    {\"messages\": \"\"\"Quiero calcular la suma de 1234 y 5678 y multiplicar el resultado por 5. Luego quiero calcular el logaritmo natural de ese resultado.\"\"\"},\n",
    ")\n",
    "\n",
    "msgs = [msg.pretty_print() for msg in response[\"messages\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 2 con LangGraph\n",
    "\n",
    "### Preparar las tools para nuestros agentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tavily tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tavily_search = TavilySearchResults(max_results=3)\n",
    "tools = [tavily_search]\n",
    "\n",
    "# Try API without an LLM\n",
    "search_results = tavily_search.invoke(\"¿Qué temperatura hace en Madrid hoy?\")\n",
    "display(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(\n",
    "    lang=\"es\", \n",
    "    top_k_results=2,\n",
    "    # doc_content_chars_max=1000\n",
    ")\n",
    "wikipedia_tool = WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [tavily_search, wikipedia_tool]\n",
    "# tools = [wikipedia_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "response = llm_with_tools.invoke(\"Hola!\")\n",
    "\n",
    "print(f\"ContentString: {response.content}\")\n",
    "print(f\"ToolCalls: {response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_with_tools.invoke(\"¿Qué temperatura hace en Madrid?\")\n",
    "\n",
    "print(f\"ContentString: {response.content}\")\n",
    "print(f\"ToolCalls: {response.tool_calls}\")\n",
    "\n",
    "# Todavía no estamos llamando a la Tool, solo nos está diciendo que lo hagamos. Para llamarlo, necesitamos crear nuestro agente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langgraph -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# We use the model, not the model_with_tools, because the agent will handle the tool calls for us.\n",
    "memory = MemorySaver()\n",
    "agent_executor = create_react_agent(llm, tools, checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abcd1234\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_executor.invoke(\n",
    "    {\"messages\": \"Hola!\"},\n",
    "    config\n",
    ")\n",
    "\n",
    "response[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = agent_executor.invoke(\n",
    "    {\"messages\": \"¿Quién es el actual presidente de Portugal?\"},\n",
    "    config\n",
    ")\n",
    "\n",
    "msgs = [msg.pretty_print() for msg in response[\"messages\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = agent_executor.invoke(\n",
    "    {\"messages\": \"Busca en internet la fecha y hora de ahora mismo en Madrid.\"},\n",
    "    config\n",
    ")\n",
    "\n",
    "msgs = [msg.pretty_print() for msg in response[\"messages\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = agent_executor.invoke(\n",
    "    {\"messages\": \"¿Va a llover mañana en Madrid?\"},\n",
    "    config\n",
    ")\n",
    "\n",
    "msgs = [msg.pretty_print() for msg in response[\"messages\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = agent_executor.invoke(\n",
    "    {\"messages\": \"¿Va a llover mañana en Madrid? Busca primero la fecha actual para dar el resultado correcto\"},\n",
    "    {\"configurable\": {\"thread_id\": \"nueva_key\"}}\n",
    ")\n",
    "\n",
    "msgs = [msg.pretty_print() for msg in response[\"messages\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preguntas concatenadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "agent_executor = create_react_agent(llm, tools, checkpointer=memory, debug=False)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abcd1234\"}}\n",
    "\n",
    "response = agent_executor.invoke(\n",
    "    {\"messages\": \"¿Cómo se llama la mujer del actor que hizo de Eddard Stark en Juego de Tronos?\"},\n",
    "    config\n",
    ")\n",
    "\n",
    "msgs = [msg.pretty_print() for msg in response[\"messages\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
